{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../n-trees/')\n",
    "sys.path.append('../utils/')\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import loss\n",
    "import models_tests\n",
    "import training_loop\n",
    "import utils\n",
    "import fast_generator\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "# dataset = datasets.ForestDataset(10, 2, temp=1, maxiter=10000, size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset, '../datasets/dataset_15_2_1e5.pt')\n",
    "# dataset = torch.load('../datasets/dataset_6_1_1e5.pt')\n",
    "dataset = torch.load('../datasets/dataset_10_2_1e6.pt')\n",
    "# dataset = torch.load('../datasets/dataset_15_2_1e5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.conditional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "# loss_fn = loss.EntropyLoss(P_factor=15)\n",
    "loss_fn = loss.ConditionalEntropyLoss(P_factor=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "model = models_tests.ConditionalEntropyPrecond(\n",
    "    n=dataset.n,\n",
    "    d=dataset.d,\n",
    "    model =  models_tests.ConditionalAttention(dataset.n, \n",
    "                                        dataset.d,                          # Number of color channels at input.\n",
    "                                        dropout             = 0.10,         # Dropout probability of intermediate activations.\n",
    "                                        num_heads           = 16,            # Number of layers in the MLP.\n",
    "                                        model_dimension     = 512,          # Hidden layer size.\n",
    "                                        num_encoder_layers  = 6,\n",
    "                                        embedding_type      = 'fourier', # Timestep embedding type: 'positional' for DDPM++, 'fourier' for NCSN++.\n",
    "                                        embedding_channels  = 128,          # Number of channels in the timestep embedding.\n",
    "                                    )\n",
    ").to(device)\n",
    "\n",
    "# model = models_tests.EntropyPrecond(\n",
    "#     n=dataset.n,\n",
    "#     d=dataset.d,\n",
    "#     model =  models_tests.Attention(dataset.n, \n",
    "#                                     dataset.d,                          # Number of color channels at input.\n",
    "#                                     dropout             = 0.10,         # Dropout probability of intermediate activations.\n",
    "#                                     num_heads           = 16,            # Number of layers in the MLP.\n",
    "#                                     model_dimension     = 512,          # Hidden layer size.\n",
    "#                                     num_encoder_layers  = 6,\n",
    "#                                     embedding_type      = 'fourier',    # Timestep embedding type: 'positional' for DDPM++, 'fourier' for NCSN++.\n",
    "#                                     embedding_channels  = 128,          # Number of channels in the timestep embedding.\n",
    "#                                     )\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 73.438MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.000001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 10, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset[0][0].repeat(4,1,1,1).to(device), torch.randn(4,10,10,10).to(device), torch.tensor([0.5, 0.5, 0.5, 0.5]).to(device)).to(device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/miniconda3/envs/ntrees/lib/python3.11/site-packages/torch/utils/data/sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing network...\n",
      "Setting up optimizer...\n",
      "Setting up logs...\n",
      "Training for 2097152 kimg...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "training_loop.simple_training_loop(\n",
    "    run_dir             = '../experiments/test16_attention_conditional',      # Output directory.\n",
    "    dataset             = dataset,      # Options for training set.\n",
    "    network             = model,        # Options for model and preconditioning.\n",
    "    loss                = loss_fn,      # Options for loss function.\n",
    "    optimizer           = optimizer,    # Options for optimizer.\n",
    "    seed                = 0,            # Global random seed.\n",
    "    batch_size          = 1024,          # Total batch size for one training iteration.\n",
    "    num_workers         = 16,           # Number of data loading workers.\n",
    "    total_kimg          = 1024<<11,      # Training duration, measured in thousands of training images.\n",
    "    device              = device,\n",
    "    kimg_per_tick       = 1024<<1,          # How often to save the training state.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('../experiments/test01/training-state-100000.pt')['net'].to(device).eval()\n",
    "# model = torch.load('../experiments/test05/training-state-100000.pt')['net'].to(device).eval()\n",
    "\n",
    "# model = torch.load('../experiments/test12_attention/training-state-680000.pt')['net'].to(device).eval()\n",
    "model = torch.load('../experiments/test12_attention/training-state-680000.pt')['net'].to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ForestDataset(10, 2, 1, 10000, 100)\n",
    "# test_dataset = datasets.ForestDataset(6, 1, 1, 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.load('../datasets/dataset_10_2_1e6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, prior = test_dataset[0]\n",
    "data = data.to(device).to(torch.float32).reshape(1, data.shape[0], data.shape[1])\n",
    "prior = prior.to(device).to(torch.float32).reshape(1, prior.shape[0], prior.shape[1], prior.shape[2])\n",
    "\n",
    "rnd_normal = torch.randn([data.shape[0],1,1], device=data.device) * 1\n",
    "p = rnd_normal.exp()\n",
    "p = (p / 20).clamp(0, 1) / 2 # clamp flip probability to [0,0.5]\n",
    "\n",
    "# beta = torch.distributions.beta.Beta(1.3, 4)\n",
    "# p = beta.sample([data.shape[0],1,1]).to(data.device) / 2\n",
    "\n",
    "\n",
    "weight = 1 / (2 * p) # weight for loss function for balancing preconditioning loss potentially not needed\n",
    "y = data\n",
    "n = torch.bernoulli(torch.ones_like(y) * (1 - p)) * 2 - 1 # noise equal to a bit flip occuring with probability p\n",
    "n = n.to(torch.float32)\n",
    "D_yn = model(y * n, prior, p)\n",
    "\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 6, figsize=(25, 5))\n",
    "\n",
    "ax[0].imshow(data[0].cpu().numpy())\n",
    "ax[1].imshow(n[0].cpu().numpy())\n",
    "ax[2].imshow((y * n)[0].cpu().numpy())\n",
    "ax[3].imshow(D_yn[0].detach().cpu().numpy())\n",
    "ax[4].imshow((D_yn[0].detach()-data[0]).cpu().numpy(), vmin=-0.5, vmax=0.5)\n",
    "ax[5].imshow((torch.argmax(prior[0], dim=0).detach()).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropic_sampler(net, latents, priors=None, num_steps=100, churn = 1, p_min = 0.001, rho=4):\n",
    "    \n",
    "    # Time step discretization.\n",
    "    step_indices = torch.arange(num_steps, dtype=torch.float64, device=latents.device)\n",
    "    t_steps = (0.5 ** (1 / rho) + step_indices / (num_steps - 1) * (p_min ** (1 / rho) - 0.5 ** (1 / rho))) ** rho\n",
    "    t_steps = torch.cat([torch.as_tensor(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
    "    \n",
    "    if priors is not None:\n",
    "        priors = priors.to(torch.float64).to(latents.device)\n",
    "    \n",
    "    x_next = latents.to(torch.float64)\n",
    "    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):\n",
    "        x_cur = x_next\n",
    "\n",
    "        if churn > 0: # Removal of churn reduces this to Euler-Maruyama, a churn of 1 is approximately 1% noising at each step\n",
    "            gamma = churn / num_steps if p_min <= t_cur <= 0.5 else 0\n",
    "            t_hat = torch.as_tensor(t_cur + gamma * t_cur)\n",
    "            p_churn = torch.bernoulli(torch.ones_like(latents) * (t_hat - t_cur)) * 2 - 1 # here probabilities follow a simple sum\n",
    "            x_hat = x_cur * p_churn\n",
    "        else:\n",
    "            t_hat = t_cur\n",
    "            x_hat = x_cur\n",
    "            \n",
    "\n",
    "        # Euler step.\n",
    "        if priors is not None:\n",
    "            denoised = net(x_hat, priors, t_hat).to(torch.float64)\n",
    "        else:\n",
    "            denoised = net(x_hat, t_hat).to(torch.float64)\n",
    "        n = torch.bernoulli(((denoised * x_hat * (t_next - 1)) + 1) / 2) * 2 - 1 # flip probability computed with product\n",
    "        x_next = x_hat * n\n",
    "\n",
    "        yield utils.EasyDict(x=x_next, denoised=denoised)\n",
    "    # return x_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = 6\n",
    "d = 10\n",
    "latents = torch.bernoulli(torch.ones((1,d,d), device=device) * 0.5) * 2 - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = test_dataset[15][1].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = entropic_sampler(model, latents, priors=prior, num_steps=50, churn = 0.01, p_min = 0.0001, rho=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(next(sampler).denoised.detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    forest = np.stack((((next(sampler).denoised.detach().cpu().numpy()[0]+1.3)/2).astype(np.int64), (torch.argmax(prior[0], dim=0).detach()).abs().cpu().numpy()))\n",
    "fast_generator.plot_forest(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p sample weighting, log-normal\n",
    "rnd_normal = torch.randn([100000]) * 1.2\n",
    "p = rnd_normal.exp()\n",
    "p = (p / 20).clamp(0, 1)\n",
    "plt.hist(p, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p sample weighting, log-normal\n",
    "rnd_normal = torch.randn([100000]) * 1.2\n",
    "p = rnd_normal.exp()\n",
    "p = (p / 15).clamp(0, 1) / 2 # clamp flip probability to [0,0.5]\n",
    "plt.hist(p, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.distributions.beta.Beta(1.5, 10)\n",
    "p = beta.sample([100000])\n",
    "plt.hist(p, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_normal = torch.randn([100000]) * 1.2\n",
    "p = rnd_normal.exp()\n",
    "p = (p / 200).clamp(0, 1)\n",
    "plt.hist(p, bins=500)\n",
    "plt.show()\n",
    "torch.mean(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.distributions.beta.Beta(10, 2)\n",
    "p = beta.sample([100000])\n",
    "plt.hist(p, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.rand(100000) / 2\n",
    "plt.hist(p, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntrees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
